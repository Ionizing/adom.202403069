#!/bin/bash
#SBATCH --partition       h3cm
#SBATCH --time            24:00:00
#SBATCH --nodes           1
#SBATCH --ntasks-per-node 48
#SBATCH --cpus-per-task   1
##SBATCH --error           out_%j.err
##SBATCH --output          out_%j.log

ulimit -s unlimited

#module load vasp/5.4.4
#module load vasp/5.4.4_soc         # sugon ver avail
#module load vasp/6.3.0_wannier90v3  # sugon ver avail
module load vasp/6.3.0_wan_soc_vtst  # sugon ver avail
#module load vasp/6.1.0_vtst        # sugon ver avail
#module load vasp/6.1.0_omp
#module load vasp/6.1.0_constr_z
#module load vasp/6.1.0_optcell   # see http://blog.wangruixing.cn/2019/05/05/constr for usage

#export OMP_NUM_THREADS=8
#export I_MPI_PMI_LIBRARY=libpmi2.so

echo "============================================================"
module list
env | grep "MKLROOT="
echo "============================================================"
echo "Job ID: $SLURM_JOB_NAME"
echo "Job name: $SLURM_JOB_NAME"
echo "Number of nodes: $SLURM_JOB_NUM_NODES"
echo "Number of processors: $SLURM_NTASKS"
echo "Task is running on the following nodes:"
echo $SLURM_JOB_NODELIST
echo "OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK"
echo "============================================================"
echo

export HDF5_USE_FILE_LOCKING="FALSE"   # see https://forum.hdfgroup.org/t/hdf5-files-on-nfs/3985/7 for reason

#srun vasp_std
#srun vasp_std_sugon # to run sugon version
#srun vasp_gam
srun vasp_ncl
